{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56ea34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved layer names:\n",
      "bidirectional\n",
      "breathing_attention\n",
      "conv1d\n",
      "dense\n",
      "dense_1\n",
      "dense_2\n",
      "dropout\n",
      "dropout_1\n",
      "dropout_2\n",
      "input_1\n",
      "layer_normalization\n",
      "layer_normalization_1\n",
      "max_pooling1d\n",
      "multi_head_attention\n",
      "tf.__operators__.add\n",
      "tf.__operators__.add_1\n",
      "top_level_model_weights\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(model_path, 'r') as f:\n",
    "    print(\"Saved layer names:\")\n",
    "    for layer in f['model_weights']:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37be06b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.1\n"
     ]
    }
   ],
   "source": [
    "print(h5py.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1034575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADWIN ADI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\h5py\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(h5py.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b673c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded successfully!\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 450, 6)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 450, 64)      1984        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 225, 64)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 225, 128)     66048       ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 225, 128)    256         ['bidirectional[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 225, 128)    131968      ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 225, 128)     0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 225, 128)    0           ['dropout[0][0]',                \n",
      " da)                                                              'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 225, 128)    256         ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 225, 128)     16512       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 225, 128)     0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 225, 128)    0           ['dropout_1[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " breathing_attention (Breathing  (None, 128)         353         ['tf.__operators__.add_1[0][0]'] \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['breathing_attention[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            195         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 225,828\n",
      "Trainable params: 225,828\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to your model\n",
    "model_path = r\"C:\\Users\\ADWIN ADI\\OneDrive\\Desktop\\Jupyter notebook projects\\RESP_MODELS\\RESP_MODEL_PROJECT_COUSTOMIZED\\breathing_model.h5\"\n",
    "\n",
    "# Load with custom object\n",
    "model = load_model(model_path, custom_objects={'BreathingAttention': BreathingAttention})\n",
    "\n",
    "print(\" Model loaded successfully!\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4217b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class BreathingAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BreathingAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply softmax across time steps (axis=1)\n",
    "        attention_weights = tf.nn.softmax(inputs, axis=1)\n",
    "        output = inputs * attention_weights\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed44a8e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 8 layers, found 9 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Register the custom layer\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mADWIN ADI\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mJupyter notebook projects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRESP_MODELS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRESP_MODEL_PROJECT_COUSTOMIZED\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbreathing_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBreathingAttention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBreathingAttention\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADWIN ADI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ADWIN ADI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\saving\\hdf5_format.py:719\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    717\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[1;32m--> 719\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    720\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    721\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    722\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[0;32m    726\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 8 layers, found 9 saved layers."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Register the custom layer\n",
    "model = load_model(r\"C:\\Users\\ADWIN ADI\\OneDrive\\Desktop\\Jupyter notebook projects\\RESP_MODELS\\RESP_MODEL_PROJECT_COUSTOMIZED\\breathing_model.h5\",custom_objects={'BreathingAttention': BreathingAttention})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0bc4dba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 8 layers, found 9 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the existing model with the custom layer\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mADWIN ADI\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mJupyter notebook projects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRESP_MODELS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRESP_MODEL_PROJECT_COUSTOMIZED\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbreathing_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBreathingAttention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBreathingAttention\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Optional: print summary to check model architecture\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\ADWIN ADI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ADWIN ADI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\saving\\hdf5_format.py:719\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    717\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[1;32m--> 719\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    720\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    721\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    722\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[0;32m    726\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 8 layers, found 9 saved layers."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load the existing model with the custom layer\n",
    "model = load_model(r\"C:\\Users\\ADWIN ADI\\OneDrive\\Desktop\\Jupyter notebook projects\\RESP_MODELS\\RESP_MODEL_PROJECT_COUSTOMIZED\\breathing_model.h5\", custom_objects={'BreathingAttention': BreathingAttention})\n",
    "\n",
    "# Optional: print summary to check model architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile again if needed\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "# Train on new data\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "# Save the updated model\n",
    "model.save(\"breathing_updated.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f04b2c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 8 layers, found 9 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_config()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Now load the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mADWIN ADI\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mJupyter notebook projects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRESP_MODELS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRESP_MODEL_PROJECT_COUSTOMIZED\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbreathing_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBreathingAttention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBreathingAttention\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADWIN ADI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ADWIN ADI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\saving\\hdf5_format.py:719\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    717\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[1;32m--> 719\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    720\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    721\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    722\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[0;32m    726\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 8 layers, found 9 saved layers."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the custom layer exactly how it was defined when saved\n",
    "class BreathingAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BreathingAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention = tf.nn.softmax(inputs, axis=1)\n",
    "        return inputs * attention\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "# Now load the model\n",
    "model = load_model(\n",
    "    r\"C:\\Users\\ADWIN ADI\\OneDrive\\Desktop\\Jupyter notebook projects\\RESP_MODELS\\RESP_MODEL_PROJECT_COUSTOMIZED\\breathing_model.h5\",\n",
    "    custom_objects={'BreathingAttention': BreathingAttention}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9411f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Breathing_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_Layer (InputLayer)    [(None, 100, 1)]          0         \n",
      "                                                                 \n",
      " LSTM_1 (LSTM)               (None, 100, 64)           16896     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 100, 64)           0         \n",
      "                                                                 \n",
      " LSTM_2 (LSTM)               (None, 100, 32)           12416     \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 100, 32)           0         \n",
      "                                                                 \n",
      " BreathingAttention (Breathi  (None, 100, 32)          0         \n",
      " ngAttention)                                                    \n",
      "                                                                 \n",
      " GlobalAvgPool (GlobalAverag  (None, 32)               0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,467\n",
      "Trainable params: 30,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Layer, Multiply\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Custom Attention Layer\n",
    "class BreathingAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BreathingAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention = tf.nn.softmax(inputs, axis=1)\n",
    "        return inputs * attention\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "# Build Model with 9 Layers\n",
    "def build_breathing_model(input_shape=(100, 1)):\n",
    "    inputs = Input(shape=input_shape, name=\"Input_Layer\")                                # Layer 1\n",
    "\n",
    "    x = LSTM(64, return_sequences=True, name=\"LSTM_1\")(inputs)                           # Layer 2\n",
    "    x = Dropout(0.3, name=\"Dropout_1\")(x)                                                 # Layer 3\n",
    "\n",
    "    x = LSTM(32, return_sequences=True, name=\"LSTM_2\")(x)                                 # Layer 4\n",
    "    x = Dropout(0.3, name=\"Dropout_2\")(x)                                                 # Layer 5\n",
    "\n",
    "    x = BreathingAttention(name=\"BreathingAttention\")(x)                                  # Layer 6\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(name=\"GlobalAvgPool\")(x)                  # Layer 7\n",
    "    x = Dense(32, activation='relu', name=\"Dense_1\")(x)                                   # Layer 8\n",
    "    outputs = Dense(3, activation='softmax', name=\"Output\")(x)                            # Layer 9\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"Breathing_Model\")\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_breathing_model()\n",
    "\n",
    "# Summary to verify\n",
    "model.summary()\n",
    "\n",
    "# Optional: Save if you want a fresh .h5 model\n",
    "# model.save(\"breathing_model_fresh_9_layers.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
